{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import codecs\n",
    "import os\n",
    "import collections\n",
    "from six.moves import cPickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLoader():\n",
    "    def __init__(self, data_dir, batch_size, seq_length, encoding='utf-8'):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.encoding = encoding\n",
    "\n",
    "        input_file = os.path.join(data_dir, \"input.txt\")\n",
    "        vocab_file = os.path.join(data_dir, \"vocab.pkl\")\n",
    "        tensor_file = os.path.join(data_dir, \"data.npy\")\n",
    "\n",
    "        if not (os.path.exists(vocab_file) and os.path.exists(tensor_file)):\n",
    "            print(\"reading text file\")\n",
    "            self.preprocess(input_file, vocab_file, tensor_file)\n",
    "        else:\n",
    "            print(\"loading preprocessed files\")\n",
    "            self.load_preprocessed(vocab_file, tensor_file)\n",
    "        self.create_batches()\n",
    "        self.reset_batch_pointer()\n",
    "\n",
    "    def preprocess(self, input_file, vocab_file, tensor_file):\n",
    "        with codecs.open(input_file, \"r\", encoding=self.encoding) as f:\n",
    "            data = f.read()\n",
    "        counter = collections.Counter(data)\n",
    "        count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "        self.chars, _ = zip(*count_pairs)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        with open(vocab_file, 'wb') as f:\n",
    "            cPickle.dump(self.chars, f)\n",
    "        self.tensor = np.array(list(map(self.vocab.get, data)))\n",
    "        np.save(tensor_file, self.tensor)\n",
    "\n",
    "    def load_preprocessed(self, vocab_file, tensor_file):\n",
    "        with open(vocab_file, 'rb') as f:\n",
    "            self.chars = cPickle.load(f)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        self.tensor = np.load(tensor_file)\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n",
    "\n",
    "    def create_batches(self):\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n",
    "\n",
    "        # When the data (tensor) is too small, let's give them a better error message\n",
    "        if self.num_batches==0:\n",
    "            assert False, \"Not enough data. Make seq_length and batch_size small.\"\n",
    "\n",
    "        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]\n",
    "        xdata = self.tensor\n",
    "        ydata = np.copy(self.tensor)\n",
    "        ydata[:-1] = xdata[1:]\n",
    "        ydata[-1] = xdata[0]\n",
    "        self.x_batches = np.split(xdata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x, y\n",
    "\n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 50 # RNN sequence length\n",
    "batch_size = 60  # minibatch size, i.e. size of data in each epoch\n",
    "num_epochs = 125 # you should change it to 50 if you want to see a relatively good results\n",
    "learning_rate = 0.002\n",
    "decay_rate = 0.97\n",
    "rnn_size = 128 # size of RNN hidden state (output dimension)\n",
    "num_layers = 2 #number of layers in the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05 00:59:30 URL:https://public.boxcloud.com/d/1/-gEkL6nVNi4rShiV3so5-3stqsUY-sxj0YJIHGxx1VAl9RE2cV6WAVngkjTOGMFgRP_NywXxGwE6FyLudpBAz-Vje8TrdClkB5jC5eZlh-DqLWdwXlaojWg7kEk6KuTFfkYxYK8fSA4deDByZje16CNmuZR2zu_K2OaBKtId2C3MdlON4D8B4d0d0eM8Sl084Qx70uLizPtp0xnCQs80yVfag_gPIyOHNN-C_TUfjftXa8xRMZjj9bEMoMfQIdK84BOlqEGRqlXrtFvQUvH3qNLYsGwZaXNcIWGPvRrAH9YyLz0dBxxSmfoqeB4yD2GiJBHRCrQaK6a2vQVBAP6BGlWuBcDEjGluIWAds0t-7MvImgq0fvndgaSnMhVy8fZUOJnm9TDDqhBxKcN5EwKZ2uM64ODs5TbTZTY6gIS-UCLaxKascPLbLHpMYW039hytb5eRRcBNllYQHHhqKGj2BTyF8UiSJnk6GwhbzXqL5xE0Xq0nIGqNL_P7tViMn94_761z4So7ENTpF5rxBf1YfS9HqhTbU3givrXQAZGK0fij9Axmfp_bdx9_zdREANEIGG6E8siwdvfBXEAg04ygE-xkkAasrYSqBE03LUbtD5CY1GPKtl4gC48jTem1RBtME1KlVyJaRr1J8j3CpVtFWSr8DZ0UqNRiBzHLqtxcQ_1xlv-RfEWVBC_ksmAZ_3d5olnBz0IU1mxwP9Pp8KbgU1zoG1oLodtAYFBE9HJSejwMvlUMIT2ltgqoB3JB_Ug4aVNVulm7j27sQ4p7w1ejEhZj2C6s_qhuqIQ7T4TpiKnblkqP6mXIaqGv_sWiNkvmpikM3ez1WwZDcKj0g6GF4Zxk_a56cqt7cCctcmbW_kZ6Ug0dVT2ORQlwl_6dXFNWKCIdonuTHkKhIgaB4rwJvZ3CQYjNkcD46CEonfQ5z0ooxqf8xpZM_29yUHG2OGu9Ye2I1Mz-zqKBtHa6tj-BMVTIYIydHc3sCLHM1WGAyAKfaAMlY9lDS1Ploh91cQGeDqA5Cyz37wFvqrwTGLAvnvhzbcFNHk2e2pBZ6fwG4FbwFgkcOmy-7wUn83WMdkL5wKF0RJfOiNR8zxR0_YXqHFx36AjrG325FVXecZwSv0W5XdJXwttzVcDWNrmP8-swOEcnocWHwvSNCqBvAH0wUl1pclvUb344hxHHx8gT-zCVKzZrOgTIJz2OWB3bP1m-nIr91hNZaUf2NrXPZXSu4CAYE4zPANIWVfFH/download [1115393/1115393] -> \"input.txt\" [1]\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wget -nv -O input.txt https://ibm.box.com/shared/static/a3f9e9mbpup09toq35ut7ke3l3lf03hg.txt \n",
    "with open('input.txt', 'r') as f:\n",
    "    read_data = f.read()\n",
    "    print read_data[0:100]\n",
    "f.closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading text file\n",
      "vocabulary size: 65\n",
      "Characters: (u' ', u'e', u't', u'o', u'a', u'h', u's', u'r', u'n', u'i', u'\\n', u'l', u'd', u'u', u'm', u'y', u',', u'w', u'f', u'c', u'g', u'I', u'b', u'p', u':', u'.', u'A', u'v', u'k', u'T', u\"'\", u'E', u'O', u'N', u'R', u'S', u'L', u'C', u';', u'W', u'U', u'H', u'M', u'B', u'?', u'G', u'!', u'D', u'-', u'F', u'Y', u'P', u'K', u'V', u'j', u'q', u'x', u'z', u'J', u'Q', u'Z', u'X', u'3', u'&', u'$')\n",
      "vocab number of 'F': 49\n",
      "Character sequences (first batch): [[49  9  7 ...,  1  4  7]\n",
      " [19  4 14 ..., 14  9 20]\n",
      " [ 8 20 10 ...,  8 10 18]\n",
      " ..., \n",
      " [21  2  0 ...,  0 21  0]\n",
      " [ 9  7  7 ...,  0  2  3]\n",
      " [ 3  7  0 ...,  5  9 23]]\n"
     ]
    }
   ],
   "source": [
    "data_loader = TextLoader('', batch_size, seq_length)\n",
    "vocab_size = data_loader.vocab_size\n",
    "print \"vocabulary size:\" ,data_loader.vocab_size\n",
    "print \"Characters:\" ,data_loader.chars\n",
    "print \"vocab number of 'F':\",data_loader.vocab['F']\n",
    "print \"Character sequences (first batch):\", data_loader.x_batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  9,  7, ...,  1,  4,  7],\n",
       "       [19,  4, 14, ..., 14,  9, 20],\n",
       "       [ 8, 20, 10, ...,  8, 10, 18],\n",
       "       ..., \n",
       "       [21,  2,  0, ...,  0, 21,  0],\n",
       "       [ 9,  7,  7, ...,  0,  2,  3],\n",
       "       [ 3,  7,  0, ...,  5,  9, 23]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = data_loader.next_batch()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 50)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9,  7,  6, ...,  4,  7,  0],\n",
       "       [ 4, 14, 22, ...,  9, 20,  5],\n",
       "       [20, 10, 29, ..., 10, 18,  4],\n",
       "       ..., \n",
       "       [ 2,  0,  6, ..., 21,  0,  6],\n",
       "       [ 7,  7,  4, ...,  2,  3,  0],\n",
       "       [ 7,  0, 33, ...,  9, 23,  0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.BasicRNNCell(rnn_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_cell.output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_cell.state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=(60, 50) dtype=int32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_1:0' shape=(60, 50) dtype=int32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = stacked_cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  9,  7, ...,  1,  4,  7],\n",
       "       [19,  4, 14, ..., 14,  9, 20],\n",
       "       [ 8, 20, 10, ...,  8, 10, 18],\n",
       "       ..., \n",
       "       [21,  2,  0, ...,  0, 21,  0],\n",
       "       [ 9,  7,  7, ...,  0,  2,  3],\n",
       "       [ 3,  7,  0, ...,  5,  9, 23]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "feed_dict={input_data:x, targets:y}\n",
    "session.run(input_data, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('rnnlm', reuse=False):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # 1x65)\n",
    "    #with tf.device(\"/cpu:0\"):\n",
    "        \n",
    "    # embedding variable is initialized randomely\n",
    "    embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])  #65x128\n",
    "\n",
    "    # embedding_lookup goes to each row of input_data, and for each character in the row, finds the correspond vector in embedding\n",
    "    # it creates a 60*50*[1*128] matrix\n",
    "    # so, the first elemnt of em, is a matrix of 50x128, which each row of it is vector representing that character\n",
    "    em = tf.nn.embedding_lookup(embedding, input_data) # em is 60x50x[1*128]\n",
    "    # split: Splits a tensor into sub tensors.\n",
    "    # syntax:  tf.split(split_dim, num_split, value, name='split')\n",
    "    # it will split the 60x50x[1x128] matrix into 50 matrix of 60x[1*128]\n",
    "    inputs = tf.split(em, seq_length, 1)\n",
    "    # It will convert the list to 50 matrix of [60x128]\n",
    "    inputs = [tf.squeeze(input_, [1]) for input_ in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1233625 , -0.14391455, -0.013116  , ..., -0.04574051,\n",
       "        -0.11292589, -0.0732579 ],\n",
       "       [ 0.01130158, -0.14711592, -0.1024131 , ...,  0.09682466,\n",
       "        -0.05894173, -0.10901564],\n",
       "       [-0.00731142, -0.02620618, -0.1413168 , ...,  0.17210825,\n",
       "         0.03168036,  0.15227173],\n",
       "       ..., \n",
       "       [-0.04203579, -0.11600257, -0.02687182, ...,  0.0466167 ,\n",
       "         0.1256734 ,  0.00315903],\n",
       "       [-0.08925935,  0.03854302, -0.11006494, ..., -0.11124674,\n",
       "        -0.01411405, -0.03192475],\n",
       "       [ 0.08436356,  0.13916643, -0.04529156, ...,  0.05557318,\n",
       "         0.05172578,  0.09523539]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 50, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.10953732,  0.11556537,  0.14913632, ...,  0.07149842,\n",
       "        -0.12136988, -0.02237552],\n",
       "       [-0.03133665, -0.10970489, -0.14375091, ...,  0.07577346,\n",
       "        -0.11832098,  0.10311176],\n",
       "       [ 0.04275867,  0.16907538, -0.13553825, ...,  0.11105661,\n",
       "         0.06070592,  0.00575978],\n",
       "       ..., \n",
       "       [ 0.01130158, -0.14711592, -0.1024131 , ...,  0.09682466,\n",
       "        -0.05894173, -0.10901564],\n",
       "       [-0.11554965, -0.14424968, -0.12309102, ..., -0.17461821,\n",
       "         0.11136095,  0.17032541],\n",
       "       [ 0.04275867,  0.16907538, -0.13553825, ...,  0.11105661,\n",
       "         0.06070592,  0.00575978]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em = tf.nn.embedding_lookup(embedding, input_data)\n",
    "emp = session.run(em,feed_dict={input_data:x})\n",
    "print emp.shape\n",
    "emp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Squeeze:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_1:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_2:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_3:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_4:0' shape=(60, 128) dtype=float32>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tf.split(em, seq_length, 1)\n",
    "inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "inputs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.10953732,  0.11556537,  0.14913632, ...,  0.07149842,\n",
       "        -0.12136988, -0.02237552],\n",
       "       [ 0.1622739 , -0.05693389,  0.08251728, ...,  0.08289652,\n",
       "         0.05217007,  0.07857351],\n",
       "       [-0.11087403,  0.0579195 , -0.00084011, ...,  0.04882628,\n",
       "         0.11374922, -0.00086206],\n",
       "       ..., \n",
       "       [ 0.05012651,  0.01271051,  0.0967692 , ...,  0.04220705,\n",
       "        -0.00144315, -0.08524683],\n",
       "       [-0.03133665, -0.10970489, -0.14375091, ...,  0.07577346,\n",
       "        -0.11832098,  0.10311176],\n",
       "       [-0.12919651,  0.02734718, -0.13973653, ...,  0.0622164 ,\n",
       "         0.06787138, -0.0902569 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(inputs[0],feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/cell_0/basic_rnn_cell/Tanh_98:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/cell_0/basic_rnn_cell/Tanh_99:0' shape=(60, 128) dtype=float32>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs, new_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_state, stacked_cell,\n",
    "                                                           loop_function=None, scope='rnnlm')\n",
    "new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/cell_0/basic_rnn_cell/Tanh_1:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/cell_0/basic_rnn_cell/Tanh_3:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/cell_0/basic_rnn_cell/Tanh_5:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/cell_0/basic_rnn_cell/Tanh_7:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/cell_0/basic_rnn_cell/Tanh_9:0' shape=(60, 128) dtype=float32>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00397205,  0.02240689, -0.06027894, ..., -0.02570696,\n",
       "        -0.07564873, -0.08115882],\n",
       "       [ 0.01384854, -0.08106685,  0.08388658, ...,  0.02756775,\n",
       "         0.10648959,  0.00848107],\n",
       "       [-0.01470186,  0.04787109,  0.08990581, ..., -0.0360917 ,\n",
       "         0.02848805,  0.13181604],\n",
       "       ..., \n",
       "       [ 0.10402007, -0.00941647, -0.099663  , ...,  0.01729866,\n",
       "        -0.011406  , -0.08606934],\n",
       "       [ 0.0331465 ,  0.05297693,  0.12328503, ..., -0.00317745,\n",
       "        -0.1081135 ,  0.08157679],\n",
       "       [-0.14440858,  0.01972943, -0.03758416, ..., -0.03074711,\n",
       "        -0.05441202, -0.07565662]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_output = outputs[0]\n",
    "session.run(tf.global_variables_initializer())\n",
    "session.run(first_output,feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(3000, 128) dtype=float32>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tf.reshape(tf.concat( outputs,1), [-1, rnn_size])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(3000, 65) dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax:0' shape=(3000, 65) dtype=float32>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = tf.nn.softmax(logits)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01104834,  0.01193885,  0.0172088 , ...,  0.01370785,\n",
       "         0.01367028,  0.0173541 ],\n",
       "       [ 0.01125577,  0.01214547,  0.01707018, ...,  0.01683796,\n",
       "         0.01467508,  0.01461854],\n",
       "       [ 0.01219858,  0.01234789,  0.01418491, ...,  0.01745815,\n",
       "         0.01305982,  0.01769576],\n",
       "       ..., \n",
       "       [ 0.01009449,  0.01012842,  0.01277855, ...,  0.0175134 ,\n",
       "         0.01551311,  0.01781775],\n",
       "       [ 0.01221049,  0.0105874 ,  0.01597082, ...,  0.01661074,\n",
       "         0.01636837,  0.01460471],\n",
       "       [ 0.00936382,  0.01168642,  0.01413052, ...,  0.015823  ,\n",
       "         0.01304499,  0.01283296]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(probs,feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'rnnlm/softmax_w:0' shape=(128, 65) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/softmax_b:0' shape=(65,) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/embedding:0' shape=(65, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/kernel:0' shape=(256, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/bias:0' shape=(128,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_clip =5.\n",
    "tvars = tf.trainable_variables()\n",
    "tvars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel():\n",
    "    def __init__(self,sample=False):\n",
    "        rnn_size = 128 # size of RNN hidden state vector\n",
    "        batch_size = 60 # minibatch size, i.e. size of dataset in each epoch\n",
    "        seq_length = 50 # RNN sequence length\n",
    "        num_layers = 2 # number of layers in the RNN\n",
    "        vocab_size = 65\n",
    "        grad_clip = 5.\n",
    "        if sample:\n",
    "            print(\">> sample mode:\")\n",
    "            batch_size = 1\n",
    "            seq_length = 1\n",
    "        # The core of the model consists of an LSTM cell that processes one char at a time and computes probabilities of the possible continuations of the char. \n",
    "        basic_cell = tf.contrib.rnn.BasicRNNCell(rnn_size)\n",
    "        # model.cell.state_size is (128, 128)\n",
    "        self.stacked_cell = tf.contrib.rnn.MultiRNNCell([basic_cell] * num_layers)\n",
    "\n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, seq_length], name=\"input_data\")\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, seq_length], name=\"targets\")\n",
    "        # Initial state of the LSTM memory.\n",
    "        # The memory state of the network is initialized with a vector of zeros and gets updated after reading each char. \n",
    "        self.initial_state = stacked_cell.zero_state(batch_size, tf.float32) #why batch_size\n",
    "\n",
    "        with tf.variable_scope('rnnlm_class1'):\n",
    "            softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n",
    "            softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # 1x65\n",
    "            with tf.device(\"/gpu:0\"):\n",
    "                embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])  #65x128\n",
    "                inputs = tf.split(tf.nn.embedding_lookup(embedding, self.input_data), seq_length, 1)\n",
    "                inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "                #inputs = tf.split(em, seq_length, 1)\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "        # The value of state is updated after processing each batch of chars.\n",
    "        outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, self.initial_state, self.stacked_cell, loop_function=None, scope='rnnlm_class1')\n",
    "        output = tf.reshape(tf.concat(outputs,1), [-1, rnn_size])\n",
    "        self.logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        self.probs = tf.nn.softmax(self.logits)\n",
    "        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([self.logits],\n",
    "                [tf.reshape(self.targets, [-1])],\n",
    "                [tf.ones([batch_size * seq_length])],\n",
    "                vocab_size)\n",
    "        self.cost = tf.reduce_sum(loss) / batch_size / seq_length\n",
    "        self.final_state = last_state\n",
    "        self.lr = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),grad_clip)\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    \n",
    "    def sample(self, sess, chars, vocab, num=200, prime='The ', sampling_type=1):\n",
    "        state = sess.run(self.stacked_cell.zero_state(1, tf.float32))\n",
    "        #print state\n",
    "        for char in prime[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [state] = sess.run([self.final_state], feed)\n",
    "\n",
    "        def weighted_pick(weights):\n",
    "            t = np.cumsum(weights)\n",
    "            s = np.sum(weights)\n",
    "            return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "        ret = prime\n",
    "        char = prime[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [probs, state] = sess.run([self.probs, self.final_state], feed)\n",
    "            p = probs[0]\n",
    "\n",
    "            if sampling_type == 0:\n",
    "                sample = np.argmax(p)\n",
    "            elif sampling_type == 2:\n",
    "                if char == ' ':\n",
    "                    sample = weighted_pick(p)\n",
    "                else:\n",
    "                    sample = np.argmax(p)\n",
    "            else: # sampling_type == 1 default:\n",
    "                sample = weighted_pick(p)\n",
    "\n",
    "            pred = chars[sample]\n",
    "            ret += pred\n",
    "            char = pred\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"rnn\"):\n",
    "    model = LSTMModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370/46375 (epoch 0), train_loss = 2.001, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The I'df his him\n",
      "\n",
      "LoEd: dy hlo. and?\n",
      "Ond:\n",
      "eseriing ave\n",
      "----------------------------------\n",
      "741/46375 (epoch 1), train_loss = 1.800, time/batch = 0.008\n",
      ">> sample mode:\n",
      "The spay love. 'Twarce:\n",
      "Maktad'd sleosed centy debes a\n",
      "----------------------------------\n",
      "1112/46375 (epoch 2), train_loss = 1.712, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The druch I will\n",
      "mior your hease, theur.\n",
      "\n",
      "CAMILLO:\n",
      "Wat\n",
      "----------------------------------\n",
      "1483/46375 (epoch 3), train_loss = 1.662, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The Hespeeps churwer her doye?\n",
      "\n",
      "GLOUCESTER:\n",
      "Nancemsed:\n",
      "----------------------------------\n",
      "1854/46375 (epoch 4), train_loss = 1.629, time/batch = 0.009\n",
      ">> sample mode:\n",
      "The huth we should be there you for may here much it\n",
      "b\n",
      "----------------------------------\n",
      "2225/46375 (epoch 5), train_loss = 1.604, time/batch = 0.011\n",
      ">> sample mode:\n",
      "The fire.\n",
      "\n",
      "LEONTES:\n",
      "I wloud paris.\n",
      "\n",
      "Edrpenfatchtay ban\n",
      "----------------------------------\n",
      "2596/46375 (epoch 6), train_loss = 1.586, time/batch = 0.015\n",
      ">> sample mode:\n",
      "The drother not no fal,\n",
      "Etwaltabe\n",
      "Are refecors?\n",
      "\n",
      "First\n",
      "----------------------------------\n",
      "2967/46375 (epoch 7), train_loss = 1.571, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The Frumichly me\n",
      "hath your hels'd,\n",
      "I sholl be it bef,\n",
      "\n",
      "----------------------------------\n",
      "3338/46375 (epoch 8), train_loss = 1.559, time/batch = 0.009\n",
      ">> sample mode:\n",
      "The know no, near such but\n",
      "And some in through the\n",
      "han\n",
      "----------------------------------\n",
      "3709/46375 (epoch 9), train_loss = 1.549, time/batch = 0.010\n",
      ">> sample mode:\n",
      "The will thou alief,\n",
      "This power,\n",
      "Ence at my hour; Have\n",
      "----------------------------------\n",
      "4080/46375 (epoch 10), train_loss = 1.540, time/batch = 0.008\n",
      ">> sample mode:\n",
      "The toest!\n",
      "Do thou stand.\n",
      "\n",
      "GLOUCESTER:\n",
      "O men, and much\n",
      "----------------------------------\n",
      "4451/46375 (epoch 11), train_loss = 1.532, time/batch = 0.015\n",
      ">> sample mode:\n",
      "The bost: for I fouls of thing on to provost to peta,\n",
      "\n",
      "----------------------------------\n",
      "4822/46375 (epoch 12), train_loss = 1.525, time/batch = 0.008\n",
      ">> sample mode:\n",
      "The reason we virtue that not night do?\n",
      "\n",
      "LUCEsce:\n",
      "And \n",
      "----------------------------------\n",
      "5193/46375 (epoch 13), train_loss = 1.517, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The king to trick'd them?\n",
      "\n",
      "VIRGILA:\n",
      "Mast before, but i\n",
      "----------------------------------\n",
      "5564/46375 (epoch 14), train_loss = 1.510, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The side will fly plee.\n",
      "\n",
      "CORIOLANUS:\n",
      "Play I; to quick \n",
      "----------------------------------\n",
      "5935/46375 (epoch 15), train_loss = 1.504, time/batch = 0.009\n",
      ">> sample mode:\n",
      "The to your good lord's pins.\n",
      "\n",
      "DUKE OF YORK:\n",
      "Though no\n",
      "----------------------------------\n",
      "6306/46375 (epoch 16), train_loss = 1.498, time/batch = 0.015\n",
      ">> sample mode:\n",
      "The would meet me at acconsicmortune's love are asy, i\n",
      "----------------------------------\n",
      "6677/46375 (epoch 17), train_loss = 1.493, time/batch = 0.014\n",
      ">> sample mode:\n",
      "The chance.\n",
      "I lound by Madence all this all awn seek h\n",
      "----------------------------------\n",
      "7048/46375 (epoch 18), train_loss = 1.488, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The Escidello, behold perpice, in the being, we come: \n",
      "----------------------------------\n",
      "7419/46375 (epoch 19), train_loss = 1.483, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The time such point,\n",
      "Shapp regel:\n",
      "Alonely hand:\n",
      "For th\n",
      "----------------------------------\n",
      "7790/46375 (epoch 20), train_loss = 1.479, time/batch = 0.012\n",
      ">> sample mode:\n",
      "The Watch, help\n",
      "To Bolingbroke;\n",
      "My Glare spring\n",
      "With m\n",
      "----------------------------------\n",
      "8161/46375 (epoch 21), train_loss = 1.476, time/batch = 0.010\n",
      ">> sample mode:\n",
      "The king's men, the genrive.\n",
      "\n",
      "MONTAGUE:\n",
      "Hence have eff\n",
      "----------------------------------\n",
      "8532/46375 (epoch 22), train_loss = 1.472, time/batch = 0.008\n",
      ">> sample mode:\n",
      "The citizen of exceptune thread, his grosous:\n",
      "I thank \n",
      "----------------------------------\n",
      "8903/46375 (epoch 23), train_loss = 1.469, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The omeniss;\n",
      "Sea up all so:'s honour:\n",
      "Anole.\n",
      "\n",
      "MARIANA:\n",
      "----------------------------------\n",
      "9274/46375 (epoch 24), train_loss = 1.466, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The houdd; owfice till me will reside thime, peace,\n",
      "Pr\n",
      "----------------------------------\n",
      "9645/46375 (epoch 25), train_loss = 1.464, time/batch = 0.010\n",
      ">> sample mode:\n",
      "The knot!\n",
      "Take it!\n",
      "\n",
      "ISABELLA:\n",
      "Nay, would revengest of \n",
      "----------------------------------\n",
      "10016/46375 (epoch 26), train_loss = 1.462, time/batch = 0.008\n",
      ">> sample mode:\n",
      "The counter, provo's wilt trick?\n",
      "Thereth rish again, a\n",
      "----------------------------------\n",
      "10387/46375 (epoch 27), train_loss = 1.460, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The absolves to me at madvele spirit worshion,\n",
      "That ha\n",
      "----------------------------------\n",
      "10758/46375 (epoch 28), train_loss = 1.458, time/batch = 0.008\n",
      ">> sample mode:\n",
      "The busy sworn in his,\n",
      "And be done, madam of here of S\n",
      "----------------------------------\n",
      "11129/46375 (epoch 29), train_loss = 1.457, time/batch = 0.009\n",
      ">> sample mode:\n",
      "The cally owe will what night lapt you her from thy bi\n",
      "----------------------------------\n",
      "11500/46375 (epoch 30), train_loss = 1.455, time/batch = 0.010\n",
      ">> sample mode:\n",
      "The could on me; I see the name and delave more cursio\n",
      "----------------------------------\n",
      "11871/46375 (epoch 31), train_loss = 1.454, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The thimiciles?\n",
      "\n",
      "KING HENRY VI:\n",
      "My, whats'\n",
      "Mirnce sass\n",
      "----------------------------------\n",
      "12242/46375 (epoch 32), train_loss = 1.452, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The will, that know, Rieect\n",
      "I hear your return of seen\n",
      "----------------------------------\n",
      "12613/46375 (epoch 33), train_loss = 1.450, time/batch = 0.012\n",
      ">> sample mode:\n",
      "The owed: thy verroo our eyes solesty,\n",
      "That is almost.\n",
      "----------------------------------\n",
      "12984/46375 (epoch 34), train_loss = 1.449, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The washants shall be shall I presence, a more, I thou\n",
      "----------------------------------\n",
      "13355/46375 (epoch 35), train_loss = 1.447, time/batch = 0.008\n",
      ">> sample mode:\n",
      "The childrenards,\n",
      "Your husband regree of epose.\n",
      "\n",
      "DUKE \n",
      "----------------------------------\n",
      "13726/46375 (epoch 36), train_loss = 1.446, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The seft; then fit, our boist morn of voice?\n",
      "\n",
      "STAN EDW\n",
      "----------------------------------\n",
      "14097/46375 (epoch 37), train_loss = 1.445, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The hent:\n",
      "May the knew with all the offence: offiders \n",
      "----------------------------------\n",
      "14468/46375 (epoch 38), train_loss = 1.444, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The touch'd swear my life as other was this weepars' C\n",
      "----------------------------------\n",
      "14839/46375 (epoch 39), train_loss = 1.442, time/batch = 0.008\n",
      ">> sample mode:\n",
      "The infected.\n",
      "\n",
      "SICINIUS:\n",
      "'Tis tale to water full as lo\n",
      "----------------------------------\n",
      "15210/46375 (epoch 40), train_loss = 1.441, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The lord, I speakly, your morning to fire, by your bri\n",
      "----------------------------------\n",
      "15581/46375 (epoch 41), train_loss = 1.440, time/batch = 0.008\n",
      ">> sample mode:\n",
      "The blot\n",
      "As I'll queen, Oxford i' the noble grant:\n",
      "Edw\n",
      "----------------------------------\n",
      "15952/46375 (epoch 42), train_loss = 1.439, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The grace do you to peratime art. Nay, for yat in the \n",
      "----------------------------------\n",
      "16323/46375 (epoch 43), train_loss = 1.438, time/batch = 0.011\n",
      ">> sample mode:\n",
      "The Engoo, and you were,\n",
      "Will it deceive the been upon\n",
      "----------------------------------\n",
      "16694/46375 (epoch 44), train_loss = 1.437, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The oath and eyes signsing aries me\n",
      "sayn'd a ordahos f\n",
      "----------------------------------\n",
      "17065/46375 (epoch 45), train_loss = 1.436, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The dedistip?\n",
      "\n",
      "VIRGILIA:\n",
      "Come; 'twere revenge.\n",
      "\n",
      "JULIET\n",
      "----------------------------------\n",
      "17436/46375 (epoch 46), train_loss = 1.435, time/batch = 0.008\n",
      ">> sample mode:\n",
      "The graced sin.\n",
      "\n",
      "ELBOW:\n",
      "But look you gous on!\n",
      "\n",
      "Clown:\n",
      "\n",
      "----------------------------------\n",
      "17807/46375 (epoch 47), train_loss = 1.434, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The sar mine, to do you got his house of like aftergen\n",
      "----------------------------------\n",
      "18178/46375 (epoch 48), train_loss = 1.434, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The Lord Angelo:\n",
      "Be must trust her,\n",
      "So surves, like th\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18549/46375 (epoch 49), train_loss = 1.433, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The night! Kate creeting wisdon?\n",
      "Rumement set how hear\n",
      "----------------------------------\n",
      "18920/46375 (epoch 50), train_loss = 1.432, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The cell, she stir and with a ease unto mercy, the\n",
      "me \n",
      "----------------------------------\n",
      "19291/46375 (epoch 51), train_loss = 1.432, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The did\n",
      "Derpt my grock:\n",
      "After I corn'd all\n",
      "thy fight, \n",
      "----------------------------------\n",
      "19662/46375 (epoch 52), train_loss = 1.431, time/batch = 0.008\n",
      ">> sample mode:\n",
      "The fell you;\n",
      "I' that what might to the live upon hath\n",
      "----------------------------------\n",
      "20033/46375 (epoch 53), train_loss = 1.430, time/batch = 0.010\n",
      ">> sample mode:\n",
      "The did Ksee have nours. Take their and me: the other \n",
      "----------------------------------\n",
      "20404/46375 (epoch 54), train_loss = 1.430, time/batch = 0.009\n",
      ">> sample mode:\n",
      "The proportion to well-bens!\n",
      "What has court\n",
      "Of uncle, \n",
      "----------------------------------\n",
      "20775/46375 (epoch 55), train_loss = 1.429, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The direscest. Well, me lets, where was me,\n",
      "No, are wr\n",
      "----------------------------------\n",
      "21146/46375 (epoch 56), train_loss = 1.429, time/batch = 0.010\n",
      ">> sample mode:\n",
      "The hold.\n",
      "She king; therefore, keen as a strength.\n",
      "\n",
      "AU\n",
      "----------------------------------\n",
      "21517/46375 (epoch 57), train_loss = 1.428, time/batch = 0.009\n",
      ">> sample mode:\n",
      "The agries\n",
      "Well to been wash,\n",
      "Were I save waste the ea\n",
      "----------------------------------\n",
      "21888/46375 (epoch 58), train_loss = 1.428, time/batch = 0.011\n",
      ">> sample mode:\n",
      "The may any dead, poy from as until.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "----------------------------------\n",
      "22259/46375 (epoch 59), train_loss = 1.427, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The well; I hadf and,\n",
      "Are not of scorrous looks know\n",
      "T\n",
      "----------------------------------\n",
      "22630/46375 (epoch 60), train_loss = 1.427, time/batch = 0.009\n",
      ">> sample mode:\n",
      "The truth, thinks my other and hapgem,\n",
      "His beholy more\n",
      "----------------------------------\n",
      "23001/46375 (epoch 61), train_loss = 1.426, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The least world upon his near; the offeits each'd hope\n",
      "----------------------------------\n",
      "23372/46375 (epoch 62), train_loss = 1.426, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The kings and blother! plower'd glory's heart: what th\n",
      "----------------------------------\n",
      "23743/46375 (epoch 63), train_loss = 1.425, time/batch = 0.008\n",
      ">> sample mode:\n",
      "The willing,\n",
      "And hear you present of Seconders: call'd\n",
      "----------------------------------\n",
      "24114/46375 (epoch 64), train_loss = 1.425, time/batch = 0.009\n",
      ">> sample mode:\n",
      "The day's\n",
      "may do you bed, and Sombard's\n",
      "Than thou art \n",
      "----------------------------------\n",
      "24485/46375 (epoch 65), train_loss = 1.425, time/batch = 0.010\n",
      ">> sample mode:\n",
      "The bexam,\n",
      "Our choose, wish once temples\n",
      "To greedful i\n",
      "----------------------------------\n",
      "24856/46375 (epoch 66), train_loss = 1.424, time/batch = 0.010\n",
      ">> sample mode:\n",
      "The cioled's royal citis; marry no sawful in the man's\n",
      "----------------------------------\n",
      "25227/46375 (epoch 67), train_loss = 1.424, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The climes far the laste, this.\n",
      "\n",
      "VIRGILIA:\n",
      "\n",
      "PARIS:\n",
      "The\n",
      "----------------------------------\n",
      "25598/46375 (epoch 68), train_loss = 1.423, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The cap your firs biilt thou art this in a sabse; fort\n",
      "----------------------------------\n",
      "25969/46375 (epoch 69), train_loss = 1.423, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The dirstrong at thee,\n",
      "I tell thy prosperia in in agai\n",
      "----------------------------------\n",
      "26340/46375 (epoch 70), train_loss = 1.423, time/batch = 0.015\n",
      ">> sample mode:\n",
      "The one misery, rie, Well thou malicish kill;\n",
      "But the \n",
      "----------------------------------\n",
      "26711/46375 (epoch 71), train_loss = 1.422, time/batch = 0.018\n",
      ">> sample mode:\n",
      "The die but are the powsing want ready;\n",
      "And this very \n",
      "----------------------------------\n",
      "27082/46375 (epoch 72), train_loss = 1.422, time/batch = 0.016\n",
      ">> sample mode:\n",
      "The good only penity.\n",
      "Write ilst Signior.\n",
      "\n",
      "DUKE VINCEN\n",
      "----------------------------------\n",
      "27453/46375 (epoch 73), train_loss = 1.422, time/batch = 0.016\n",
      ">> sample mode:\n",
      "The other even that I will.\n",
      "My well, bring off honours\n",
      "----------------------------------\n",
      "27824/46375 (epoch 74), train_loss = 1.421, time/batch = 0.013\n",
      ">> sample mode:\n",
      "The hatre-wisk.\n",
      "\n",
      "LEONTES:\n",
      "I would she not straight,\n",
      "Yo\n",
      "----------------------------------\n",
      "28195/46375 (epoch 75), train_loss = 1.421, time/batch = 0.018\n",
      ">> sample mode:\n",
      "The children are fair.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Pardon and kn\n",
      "----------------------------------\n",
      "28566/46375 (epoch 76), train_loss = 1.421, time/batch = 0.010\n",
      ">> sample mode:\n",
      "The heissesu'd 't!\n",
      "\n",
      "MENENIUS:\n",
      "You, where prove\n",
      "The hou\n",
      "----------------------------------\n",
      "28937/46375 (epoch 77), train_loss = 1.420, time/batch = 0.018\n",
      ">> sample mode:\n",
      "The runt this issue to her foe, Expurat showing fault,\n",
      "----------------------------------\n",
      "29308/46375 (epoch 78), train_loss = 1.420, time/batch = 0.015\n",
      ">> sample mode:\n",
      "The of execute tears that bO, if he did!\n",
      "I see out wou\n",
      "----------------------------------\n",
      "29679/46375 (epoch 79), train_loss = 1.420, time/batch = 0.015\n",
      ">> sample mode:\n",
      "The nate, this had their love of Lancaster strength th\n",
      "----------------------------------\n",
      "30050/46375 (epoch 80), train_loss = 1.419, time/batch = 0.030\n",
      ">> sample mode:\n",
      "The chearth, his sight is exquain deloved\n",
      "She was assi\n",
      "----------------------------------\n",
      "30421/46375 (epoch 81), train_loss = 1.419, time/batch = 0.029\n",
      ">> sample mode:\n",
      "The crown,\n",
      "And his prighame.\n",
      "\n",
      "POLIXENES:\n",
      "God this this\n",
      "----------------------------------\n",
      "30792/46375 (epoch 82), train_loss = 1.419, time/batch = 0.047\n",
      ">> sample mode:\n",
      "The cembroke of live:' musicle, for with sappway injur\n",
      "----------------------------------\n",
      "31163/46375 (epoch 83), train_loss = 1.418, time/batch = 0.030\n",
      ">> sample mode:\n",
      "The thou.\n",
      "\n",
      "GLOUCESTER:\n",
      "Well but being king, this will \n",
      "----------------------------------\n",
      "31534/46375 (epoch 84), train_loss = 1.418, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The citizen\n",
      "Than smile the old:\n",
      "Go to the king cares, \n",
      "----------------------------------\n",
      "31905/46375 (epoch 85), train_loss = 1.418, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The tomby! I fay.\n",
      "\n",
      "ROMEO:\n",
      "Have no straight ap coliffar\n",
      "----------------------------------\n",
      "32276/46375 (epoch 86), train_loss = 1.418, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The cersiness,\n",
      "Cathery.\n",
      "\n",
      "WARWICK:\n",
      "Speak--what?\n",
      "\n",
      "PETRUC\n",
      "----------------------------------\n",
      "32647/46375 (epoch 87), train_loss = 1.417, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The think'st with the sword precerer; I scian off prin\n",
      "----------------------------------\n",
      "33018/46375 (epoch 88), train_loss = 1.417, time/batch = 0.008\n",
      ">> sample mode:\n",
      "The son, my boy, to murdy fraged her flowing myself me\n",
      "----------------------------------\n",
      "33389/46375 (epoch 89), train_loss = 1.417, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The gond light married, he might codpersolman\n",
      "Is shall\n",
      "----------------------------------\n",
      "33760/46375 (epoch 90), train_loss = 1.417, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The pricknes-folly to the city with out.\n",
      "\n",
      "GREMIO:\n",
      "No, \n",
      "----------------------------------\n",
      "34131/46375 (epoch 91), train_loss = 1.416, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The Gead?\n",
      "What, I am to-morrow:\n",
      "Therefore I light in y\n",
      "----------------------------------\n",
      "34502/46375 (epoch 92), train_loss = 1.416, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The chibivings seeds at seems is own rouble of the god\n",
      "----------------------------------\n",
      "34873/46375 (epoch 93), train_loss = 1.416, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The her\n",
      "True thought quainted;\n",
      "For ovin his pp me look\n",
      "----------------------------------\n",
      "35244/46375 (epoch 94), train_loss = 1.416, time/batch = 0.008\n",
      ">> sample mode:\n",
      "The kill, but I do no hole,\n",
      "Therefore; soft\n",
      "Embea,\n",
      "I w\n",
      "----------------------------------\n",
      "35615/46375 (epoch 95), train_loss = 1.415, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The great son and hear this one amender;\n",
      "And it\n",
      "that m\n",
      "----------------------------------\n",
      "35986/46375 (epoch 96), train_loss = 1.415, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The deny's fearful both. O, folsty of this bick of tru\n",
      "----------------------------------\n",
      "36357/46375 (epoch 97), train_loss = 1.415, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The well, Virstress'd as war so dear. Two both\n",
      "With ho\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36728/46375 (epoch 98), train_loss = 1.415, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The judgry, oment for my side him was did by Glayted o\n",
      "----------------------------------\n",
      "37099/46375 (epoch 99), train_loss = 1.415, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The secutions will from her,\n",
      "Will have bear my feeging\n",
      "----------------------------------\n",
      "37470/46375 (epoch 100), train_loss = 1.414, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The obdond good slown his fender reported by war befki\n",
      "----------------------------------\n",
      "37841/46375 (epoch 101), train_loss = 1.414, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The woull nease' of not made\n",
      "By at, in my mouths,\n",
      "To c\n",
      "----------------------------------\n",
      "38212/46375 (epoch 102), train_loss = 1.414, time/batch = 0.010\n",
      ">> sample mode:\n",
      "The old eveny danger'd of the treatic God!\n",
      "They sport \n",
      "----------------------------------\n",
      "38583/46375 (epoch 103), train_loss = 1.414, time/batch = 0.010\n",
      ">> sample mode:\n",
      "The wounds me night;\n",
      "My sadient himself, what tell as \n",
      "----------------------------------\n",
      "38954/46375 (epoch 104), train_loss = 1.414, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The cleast--Where I never nuffend so learness the gods\n",
      "----------------------------------\n",
      "39325/46375 (epoch 105), train_loss = 1.414, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The were sometimung abservadion waint us buy, for inkl\n",
      "----------------------------------\n",
      "39696/46375 (epoch 106), train_loss = 1.413, time/batch = 0.008\n",
      ">> sample mode:\n",
      "The isaured lips, sir: Montagues gill'd Cheporret on\n",
      "m\n",
      "----------------------------------\n",
      "40067/46375 (epoch 107), train_loss = 1.413, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The thousand\n",
      "you should o'er'st no aught this honest, \n",
      "----------------------------------\n",
      "40438/46375 (epoch 108), train_loss = 1.413, time/batch = 0.008\n",
      ">> sample mode:\n",
      "The signbosher, what was his soar\n",
      "All outwaige more an\n",
      "----------------------------------\n",
      "40809/46375 (epoch 109), train_loss = 1.413, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The rages,\n",
      "hang ingreated is!\n",
      "Whereof the sight.\n",
      "\n",
      "MARC\n",
      "----------------------------------\n",
      "41180/46375 (epoch 110), train_loss = 1.413, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The cereveful manise here unclires\n",
      "Dost be fool, const\n",
      "----------------------------------\n",
      "41551/46375 (epoch 111), train_loss = 1.413, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The cerepormin,\n",
      "Old you follow accuse her.\n",
      "\n",
      "DORCAS:\n",
      "Yo\n",
      "----------------------------------\n",
      "41922/46375 (epoch 112), train_loss = 1.413, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The which to die\n",
      "From a fire of less, and, learn her p\n",
      "----------------------------------\n",
      "42293/46375 (epoch 113), train_loss = 1.412, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The words; indeed smellous God,\n",
      "And prove\n",
      "In your buzz\n",
      "----------------------------------\n",
      "42664/46375 (epoch 114), train_loss = 1.412, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The dromptimely nothing boy, and, if yet thou wert eye\n",
      "----------------------------------\n",
      "43035/46375 (epoch 115), train_loss = 1.412, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The slagua;\n",
      "Then do came as he, sir, why trade your in\n",
      "----------------------------------\n",
      "43406/46375 (epoch 116), train_loss = 1.412, time/batch = 0.010\n",
      ">> sample mode:\n",
      "The Edward thee in partive him my slament place.\n",
      "\n",
      "GEOH\n",
      "----------------------------------\n",
      "43777/46375 (epoch 117), train_loss = 1.412, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The Righness-mooks the pluitenatury name?\n",
      "If you rever\n",
      "----------------------------------\n",
      "44148/46375 (epoch 118), train_loss = 1.412, time/batch = 0.008\n",
      ">> sample mode:\n",
      "The thin the law knows?\n",
      "\n",
      "QUEEN MARGARET:\n",
      "O, by Sir and\n",
      "----------------------------------\n",
      "44519/46375 (epoch 119), train_loss = 1.412, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The Marced, in my heirs, if I made mine,\n",
      "For with remo\n",
      "----------------------------------\n",
      "44890/46375 (epoch 120), train_loss = 1.412, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The Cleedy.\n",
      "Should our willous removed that uven that \n",
      "----------------------------------\n",
      "45261/46375 (epoch 121), train_loss = 1.411, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The scont;\n",
      "All har\n",
      "The way him\n",
      "Draw, young some crown \n",
      "----------------------------------\n",
      "45632/46375 (epoch 122), train_loss = 1.411, time/batch = 0.009\n",
      ">> sample mode:\n",
      "The purploy.\n",
      "\n",
      "ISABELLA:\n",
      "Dauness and away?\n",
      "Will his sta\n",
      "----------------------------------\n",
      "46003/46375 (epoch 123), train_loss = 1.411, time/batch = 0.007\n",
      ">> sample mode:\n",
      "The contemstiturs? why I did by the noble.\n",
      "Where he so\n",
      "----------------------------------\n",
      "46374/46375 (epoch 124), train_loss = 1.411, time/batch = 0.015\n",
      ">> sample mode:\n",
      "The sad Edward drawing no mark long ruffer shall be at\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(num_epochs): # num_epochs is 5 for test, but should be higher\n",
    "        sess.run(tf.assign(model.lr, learning_rate * (decay_rate ** e)))\n",
    "        data_loader.reset_batch_pointer()\n",
    "        state = sess.run(model.initial_state) # (2x[60x128])\n",
    "        for b in range(data_loader.num_batches): #for each batch\n",
    "            start = time.time()\n",
    "            x, y = data_loader.next_batch()\n",
    "            feed = {model.input_data: x, model.targets: y, model.initial_state:state}\n",
    "            train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)\n",
    "            end = time.time()\n",
    "        print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
    "                .format(e * data_loader.num_batches + b, num_epochs * data_loader.num_batches, e, train_loss, end - start))\n",
    "        with tf.variable_scope(\"rnn\", reuse=True):\n",
    "            sample_model = LSTMModel(sample=True)\n",
    "            print sample_model.sample(sess, data_loader.chars , data_loader.vocab, num=50, prime='The ', sampling_type=1)\n",
    "            print '----------------------------------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
